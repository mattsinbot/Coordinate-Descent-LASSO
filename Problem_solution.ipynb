{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LASSO Regression with Coordinate Descent Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we solve regression problem based on a synthetic dataset. Then we use an optimized version of **coordinate descent** optimization algorithm to solve LASSO regression problem. At first we will talk about how have we generated the synthetic data set, followed by crude and optimized versions of LASSO solver using coordinate descent.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LASSO is the problem of solving,\n",
    "$$\\underset{\\boldsymbol{w}, b}{arg \\, min}\\sum_i (\\boldsymbol{w}^T\\boldsymbol{x}_i + b - y_i)^2 + \\lambda \\sum_j |w_j|$$\n",
    "\n",
    "Here $\\boldsymbol{X}$ is a $d \\times n$ matrix of data, and $\\boldsymbol{x}_i$ is the $i^{th}$ column of the matrix. $\\boldsymbol{y}$ is an $n \\times 1$ vector of response variables, $\\boldsymbol{w}$ is a $d$ dimensional weight vector, $b$ is a scalar offset term and $\\lambda$ is a regularization tuning parameter.\n",
    "\n",
    "A benefit of the LASSO is that of we believe many features are irrelevant for predicting $\\boldsymbol{y}$, the LASSO can be used to enforce a sparse solution, effectively differentiating between the relevant and irrelevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating synthetic data\n",
    "Suppose that there are $k < d$ number of features that are non-zero and features $k+1$ through $d$ are unnecessary (and potentially even harmful) for predicting $\\boldsymbol{y}$.\n",
    "\n",
    "Letting $N=250$, $d = 80$, $k=10$ and $\\sigma = 1$ we generate data by drawing each element of $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times N}$ from a standard normal distribution $\\mathcal{N}(0,1)$.\n",
    "\n",
    "Also we consider $b^*=0$ and create a $\\boldsymbol{w}^*$ by setting the first $k=10$ elements to $\\pm 10$ and the remaining elements to $0$.\n",
    "\n",
    "Finally generate a Gaussian noise vector $\\epsilon$ with variance $\\sigma^2$ and form\n",
    "$$\\boldsymbol{y} = \\boldsymbol{X}^T\\boldsymbol{w}^* + b^* + \\epsilon$$\n",
    "\n",
    "With this synthetic data we solve multiple LASSO problems on a regularization path, starting at $\\lambda_{max}$ and decreasing $\\lambda$ by a constant ration of $2$ for $10$ steps.\n",
    "\n",
    "To compute $\\lambda_{max}$, the smallest value of $\\lambda$ for which the solution $\\boldsymbol{\\hat{w}}$ is entirely zero is given by\n",
    "$$\\lambda_{max} = 2 \\|\\boldsymbol{X}\\left(\\boldsymbol{y} - \\frac{1}{n}\\sum_i y_i\\right)\\|_{\\infty}$$\n",
    "The above equation helps to choose the first $\\lambda$ in a regularization path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinate Descent for LASSO\n",
    "The brute force way to implement coordinate descent is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"img/lasso1.png\",width=15,height=15>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"img/lasso1.png\",width=15,height=15>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However we can reduce time complexity by avoiding the repeated expensive computations. Below I have presented an efficient version of coordinate descent to solve LASSO which took $<1 ms$ in MATLAB format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while old_obj - new_obj > tol\n",
    "        old_obj = new_obj;\n",
    "        b = sum(data.y - data.X'*w_vec)/n;\n",
    "        for k = 1:size(data.X, 1)\n",
    "            ck = 2*data.X(k,:)*(data.y - data.X'*w_vec - b) + w_vec(k,1)*ak(1,k);\n",
    "            if ck < -lam\n",
    "                w_vec(k, 1) = (ck + lam)/ak(1, k);                \n",
    "            elseif ck > lam\n",
    "                w_vec(k, 1) = (ck - lam)/ak(1, k);                \n",
    "            else\n",
    "                w_vec(k, 1) = 0;                \n",
    "            end\n",
    "        end\n",
    "        new_obj = current_obj(data.X, data.y, b, w_vec, lam);\n",
    "        del_obj = old_obj - new_obj;\n",
    "        \n",
    "        % Stop before diverging\n",
    "        if del_obj < 0\n",
    "            break;\n",
    "        end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the correctness of the answer\n",
    "To ensure that a solution $(\\boldsymbol{\\hat{w}}, \\hat{b})$ is correct, we also need to compute the value\n",
    "$$2\\boldsymbol{X}(\\boldsymbol{X}^T\\boldsymbol{\\hat{w}} + \\hat{b} - \\boldsymbol{y})$$\n",
    "The above will resul in a $d$ dimensional vector that should take the value $-\\lambda sign(w_j)$ at $j$ for each $w_j$ that is non-zero. For the zero indices of $\\boldsymbol{\\hat{w}}$, this vector should take values lesser in magnitude than $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of meta-parameter $\\lambda$ on non-zero features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"img/non_zero_weights.png\",width=15,height=15>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"img/non_zero_weights.png\",width=15,height=15>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen from the figure that for very high values of $\\lambda$, all of the weights estimated to be zero, however with deceased values of $\\lambda$ the algorithm correctly predicted non-zero weights in $\\boldsymbol{\\hat{w}}$. Also when $\\lambda$ value went down to very small, number of nonzero weights become more and more giving wrong predictions on non-zero weights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
